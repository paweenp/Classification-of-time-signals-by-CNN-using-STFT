\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{tabularx}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Classification of time signals by CNN using STFT\\
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
% should not be used}
\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Paween Pongsomboon}
\IEEEauthorblockA{\textit{Computational Intelligence } \\
\textit{Matriculation 1346048}\\
% City, Country \\
paween.pongsomboon@stud.fra-uas.de}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Mahdieh Pirmoradian}
\IEEEauthorblockA{\textit{Computational Intelligence} \\
\textit{Matriculation 1323281}\\
%City, Country \\
mahdieh.pirmoradian@stud.fra-uas.de}
\and
\IEEEauthorblockN{3\textsuperscript{rd} TestName }
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{4\textsuperscript{th} Md Rabiul Islam}
\IEEEauthorblockA{\textit{Computational Intelligence } \\
\textit{Matriculation 1345492}\\
% City, Country \\
md.islam3@stud.fra-uas.de}
}

\maketitle

\begin{abstract}
Time dominated signal classification is an important field  that has so far covered a wide range of applications. Despite its popularity over the last few decades, it remains a difficult task that falls short of efficiency due to the nature of its  randomness and dimensionality, large in data size, and continuous updating. Classifying these time signals can be beneficial in a variety of ways but Deep learning has resulted in the development of new methods, particularly Convolutional Neural Network (CNN) models. This paper proposes a method for classifying time-dominated signals from three particular objects using a two-dimensional (2D) convolution neural network (CNN) and the Short-Time Fourier Transformation (STFT).
\end{abstract}

\begin{IEEEkeywords}
Convolutional Neural Network, Short Time Fourier Transform, Spectrogram, Classification of time signals
\end{IEEEkeywords}

\section{INTRODUCTION}
A great deal of information about our everyday surroundings and the physical actions that happens there can be transmitted by sound. Therefore, classifying this sounds can be helpful in many ways like surveillance systems or awareness and understanding of other species behaviours. But on one side, humans hearing frequency range is limited between 20 to 20,000. On the other side, handling large volumes of data is impossible or time consuming. That is when, machine learning techniques can be used. With recent great advancements in the application of automatic systems numerous researches have been done in automatic sound classification as well. For example, EnvNet \cite{b1} and speaker recognition \cite{b2}. in this research a strong deep learning machine learning approach that differs from other types of neural networks for instance having a specialty in being able to recognize patterns as well as find meaning of them names as a Convolutional Neural Network (CNN), get attentions. It behaves similarly to how our visual cortex recognizes images by Feature learning and classification. First by giving a big amount of labeled data to CNNs for training it can recognize the features which are similar in each category then at the time of testing by receiving a new data it can recognize to which category it belongs to. CNNs are consisted of some layers. It receives the input in the form of an image then in each layer there is a filter to recognize features then the output of each layer is the input to the next layer. In CNNs the first few layers are for feature extraction and the last layers for classification. 
When we try to train our CNN straight from row audio, we get certain issues. Firstly, long-range dependencies are difficult to record. Furthermore, in most devices like cellphones they use image classification functionality also. Therefore to reduce the cost of development it is better to use the same technology for both image and sound classification. That is the main reason that, using a Spectrogram, which is a far more compact representation of the sound and computationally simpler than raw audio is helpful. We utilize a spectrogram to visualize signals into images, which divides your signal into tiny windows and displays a spectrum of colors indicating the strength of each frequency. In reality, it employs the Short Time Fourier Transform (STFT). The fundamental aim of this paper is to show how we can use CNNs to categorize a long-sampled time domain signal. Then we reviewed what adjustments needed to be made to our Conv Net's design in order to achieve a better result in the accuracy of model classifying, as well as what aspects we should consider while testing our program. Furthermore, in the process of creating spectrograms, selecting the window type, size, and speed of movement was critical for obtaining an accurate picture of the sound.


\section{METHODOLOGY}

The experiment of classification of time signals by CNN using STFT has been conducted following the flowchart diagram in (Fig.~\ref{fig_flowchart}).
For the sake of simplicity, the experiment is divided into parts. The process of experiment begins with converting audio time signal into spectrogram images using STFT. Spectrogram images are then feeded into CNN model with supervised learning technique. The training process using CNN model requires the CNN model to adjust weights according to training and validated data. Lastly, the trained CNN model is then used in classification experiment.

\newpage
\begin{figure}[htbp]
\centerline{\includegraphics[scale = 0.3]{flowchart_2.png}}
\caption{Flowchart diagram of the experiment}
\label{fig_flowchart}
\end{figure}


\subsection{Dataset}
The dataset used in this experiment can be founded at \cite{b4} under "Data" directory. We use files "Data Object 1.xlsx" to "Data Object 3.xlsx" as shown in (table \ref{table_traindata}) to train the CNN model. While we use files "T File 1.xlsx" to "T File 12.xlsx" as shown in (table \ref{table_classifydata}) to do the classfication experiment. 

\begin{table}[htbp]
\centering
\caption{Dataset for training model}
\begin{tabularx}{0.48\textwidth}{p{0.25\linewidth} | p{0.25\linewidth} | p{0.3\linewidth}}
\hline
Data    & Sampling Signal & Description\\
\hline
Data Object 1.xlsx  &315 & 1-D time signal data\\
Data Object 2.xlsx  &200 & 1-D time signal data\\
Data Object 3.xlsx  &400 & 1-D time signal data\\
\hline
\end{tabularx}
\label{table_traindata}
\end{table}

\begin{table}[!htbp]
\centering
\caption{Dataset for classification}
\begin{tabularx}{0.48\textwidth}{p{0.25\linewidth} | p{0.25\linewidth} | p{0.3\linewidth}}
\hline
Data    & Sampling Signal & Description\\
\hline
T File 1.xlsx  &50 & 1-D time signal data\\
T File 2.xlsx  &50 & 1-D time signal data\\
T File 3.xlsx  &50 & 1-D time signal data\\
T File 4.xlsx  &50 & 1-D time signal data\\
T File 5.xlsx  &50 & 1-D time signal data\\
T File 6.xlsx  &50 & 1-D time signal data\\
T File 7.xlsx  &50 & 1-D time signal data\\
T File 8.xlsx  &50 & 1-D time signal data\\
T File 9.xlsx  &50 & 1-D time signal data\\
T File 10.xlsx  &50 & 1-D time signal data\\
T File 11.xlsx  &50 & 1-D time signal data\\
T File 12.xlsx  &50 & 1-D time signal data\\
\hline
\end{tabularx}
\label{table_classifydata}
\end{table}

\subsection{Preprocessing}
Preprocessing is a method to preprocess on data by converting one dimensional data (1D data) into two dimensional data (2D data). Preprocessing performs STFT on time signal data using (\ref{eqn_stft}), which is from Jont B. Allen\cite{b3}, and generates spectrogram as a result as shown in ``Fig.~\ref{fig_preprocessing}'', where the input is time signal data and the output is spectrogram. Spectrogram is a signal's visual representation. It plots the amplitude of the signal's frequency components over time. 

\begin{equation}
X(f,t) = \int_{-\infty}^{\infty} w(t-\tau)x(\tau)e^{j2\pi f\tau} \,d\tau \label{eqn_stft}
\end{equation}

where \(X(f, t) \) is the spectrogram, t is the time variable, \(w(t - \tau)\) is the shifted hamming window with the size of \(256\) sampling point, \(x(\tau)\) is the input
signal, and \(exp (j2\pi f\tau)\) is the complex exponential.

\begin{figure}[htbp]
\centerline{\includegraphics[scale = 0.40]{Preprocessing.png}}
\caption{Performing STFT to convert time signal data (Left) into Spectrogram (right)}
\label{fig_preprocessing}
\end{figure}

\subsection{Training and Testing}

In the experiment, the supervised learning has been used to train the CNN model, which is shown as (Fig. \ref{fig_CNN}). We train the model with the spectrogram images from preprocessing. By segregating 80 percent of each data from (table \ref{table_traindata}) for training and 20 percent for testing or validation, the CNN model then adjusts weights accordingly. 

\begin{figure}[!htbp]
\centerline{\includegraphics[scale = 0.45]{CNN2.png}}
\caption{CNN Layer Diagram}
\label{fig_CNN}
\end{figure}

(Fig. \ref{fig_CNN}) shows the CNN layer diagram used in this experiment, while the spectrogram image is the input and classification result is the output. CNN layers's details can be founded at (table \ref{table_CNN}).



\newpage
\begin{table}[!htbp]
\centering
\caption{CNN Layers in detail}
\begin{tabularx}{0.48\textwidth}{p{0.25\linewidth} | p{0.6\linewidth}}
\hline
Layer  & Description and size  \\
\hline
 Image Input  & 132×175×3 (width × height × depth) images with 'zerocenter' normalization \\
Convolution           & 8 filters 3×3×3 convolutions with stride [width = 1  height = 1] and padding 'same' \\
Batch Norm   & Batch normalization with 8 channels \\
ReLU                  & Rectified Linear Units  \\
Max Pooling           & 2×2 max pooling with stride [width = 2  height = 2] and padding [0  0  0  0] \\
\hline
Convolution             & 16 filters 3×3×8 convolutions with stride [1  1] and padding 'same' \\
Batch Norm     & Batch normalization with 16 channels \\
ReLU                    & Rectified Linear Units  \\
Max Pooling             & 2×2 max pooling with stride [2  2] and padding [0  0  0  0] \\
\hline
Convolution             & 32 filters 3×3×16 convolutions with stride [1  1] and padding 'same' \\
Batch Norm     & Batch normalization with 32 channels \\
ReLU                    & Rectified Linear Units  \\
Max Pooling             & 2×2 max pooling with stride [2  2] and padding [0  0  0  0] \\
\hline
Convolution             & 64 filters 3×3×32 convolutions with stride [1  1] and padding 'same' \\
Batch Norm     & Batch normalization with 64 channels \\
ReLU                    & Rectified Linear Units  \\
Max Pooling             & 2×2 max pooling with stride [2  2] and padding [0  0  0  0] \\
\hline
Convolution             & 128 filters 3×3×64 convolutions with stride [1  1] and padding 'same' \\
Batch Norm     & Batch normalization with 128 channels \\
ReLU                    & Rectified Linear Units  \\
\hline
Fully Connected         & 3 fully connected layer \\
Softmax                 & softmax \\
Classification Output   & crossentropyex with 'object1' and 2 other classes \\
\hline
\end{tabularx}%
\label{table_CNN}
\end{table}

The process of training CNN model requires optimizer to updates weights and biases, which we use Stochastic Gradient Descent with Momentum (SGDM) algorithm to minimise the loss function \cite{b7}. SGDM algorithm is shown as (\ref{eqn_SGDM})

\begin{equation}
\theta_{l+1} = \theta_l - \alpha\nabla E(\theta_l) + \gamma(\theta - \theta_{l-1}) 
\label{eqn_SGDM}
\end{equation}

Where $l$ is the iteration number, $\alpha > 0$ is the learning rate, $\theta $ is the parameter vector, and $\nabla E(\theta)$ is the loss function. $\gamma(\theta - \theta_{l-1})$ is the momentum term, which help reduce the oscillation in each iteration.

The algorithm evaluates loss function ($\nabla E(\theta)$ using the entire training set, which is denoted by Epoch value. the training algorithm requires parameters as \ref{table_parameter}.


\begin{table}[!htbp]
\centering
\caption{training parameter}
\begin{tabularx}{0.48\textwidth}{p{0.25\linewidth} | p{0.6\linewidth}}
\hline

\end{tabularx}%
\label{table_}
\end{table}


\subsection{Evaluation}

After we train CNN model, we evaluated the CNN model's performance with the following measurements derived from \cite{b5}, \cite{b6}, which uses confusion matrix to evaluate predictions and actual data. Confusion matrix consists of four values. True Positive (TP) represents the value of correct predictions of positive out of actual positive cases. False Positive (FP) represents the value of incorrect positive predictions. True Negative (TN) represents the value of correct predictions of negatives out of actual negative cases. False Negative (FN) represents the value of incorrect negative predictions.

\subsubsection{Accuracy}
Accuracy indicates the CNN model's performance on prediction. It is calculated as the ratio between correct prediction and overall prediction. The accuracy is measured by the equation from (\ref{eqn_accuracy}). 

\begin{equation}
Accuracy = \frac{TP + TN}{TP + TN + FP + FN} \\
\label{eqn_accuracy}
\end{equation}

\subsubsection{Precision}
Precision indicates the CNN model's ability in order to predict positive predictions correctly out of all positive predictions.  Precision is measured by the equation (\ref{eqn_precision}). Increasing precision means increasing correctness of the positive prediction.

\begin{equation}
Precision = \frac{TP}{TP + FP}
\label{eqn_precision}
\end{equation}

\subsubsection{Recall}
Recall is similar to precision, however recall measures the correctly positive predictions out of correct prediction. Recall is measured by the equation (\ref{eqn_recall}). Increasing recall value means minimising the incorrect predictions

\begin{equation}
Recall = \frac{TP}{TP + FN}
\label{eqn_recall}
\end{equation}

\subsubsection{F1-Score}
F1-Score involves precision and recall. It balances between both measurements. F1-Score is measured by the equation (\ref{eqn_f1score}). F1-Score is useful in the scenario where model gives high FP and FN.

\begin{equation}
F1-Score = \frac{2 \times Precision \times Recall}{Precision + Recall}
\label{eqn_f1score}
\end{equation}

\subsection{Classification Experiment}

We conducted the experiment 




\section{EXPERIMENT RESULT}





TODO 

Experiment with 100 times

Show confusion matrix 

Write about Evaluation score (Accu, Precision, etc)

Classification result

Classification result is shown as  ....
\\

\begin{tabularx}{0.48\textwidth} { 
  | >{\raggedright\arraybackslash}X 
  | >{\centering\arraybackslash}X 
  | >{\centering\arraybackslash}X 
  | >{\centering\arraybackslash}X |}
\hline
Object Name & Classified as Object 1 (\%) & Classified as Object 2 (\%) & Classified as Object 3 (\%)\\
\hline
T1  &100   &0   &0  \\
%\hline
T2  &100   &0   &0  \\
%\hline
T3  &84   &16   &0  \\
%\hline
T4  &88   &12   &0  \\
%\hline
T5  &88   &12   &0  \\
%\hline
T6  &12   &0   &88  \\
%\hline
T7  &70   &0   &30  \\
%\hline
T8  &60   &0   &40  \\
%\hline
T9  &64   &0   &36  \\
%\hline
T10  &80   &0   &20  \\
%\hline
T11  &82   &0   &18  \\
%\hline
T12  &0   &100   &0  \\
\hline
\end{tabularx}


\section{DISCUSSION AND CONCLUSION}

\begin{thebibliography}{00}
\bibitem{b1} Y. Tokozume and T. Harada, “Learning environmental sounds with end-to-end convolutional neural network,” in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017, pp. 2721–2725.

\bibitem{b2} Ravanelli, M., \& Bengio, Y. (2018). Speaker recognition from raw waveform with sincnet.
In 2018 IEEE Spoken Language Technology Workshop (SLT) (pp. 1021–1028).

\bibitem{b3} Jont B. Allen "Short Time Spectral Analysis, Synthesis, and Modification by Discrete Fourier Transform". IEEE Transactions on Acoustics, Speech, and Signal Processing. ASSP-25 (3): 235–238, June 1977.

\bibitem{b4} P. Pongsomboon, M. Pirmoradian, Md Mukit Khan, and Md Rabiul Islam "Classification-of-time-signals-by-CNN-using-STFT" Retreived from Github - https://github.com/paweenp/Classification-of-time-signals-by-CNN-using-STFT/blob/master/, 2021.

\bibitem{b5} Ahmed Fawzy Gad, "Evaluating Deep Learning Models: The Confusion Matrix, Accuracy, Precision, and Recall", published on 2020, accessed on 25 Aug 2021.

\bibitem{b6} Ajitesh Kumar, "Accuracy, Precision, Recall \& F1-Score", published on 3 September 2021, accessed on 20 September 2021.

\bibitem{b7} Kevin Patrick Murphy, "Machine Learning: a Probabilistic Perspective", The MIT Press, Cambridge, Massachusetts, 2012.




\end{thebibliography}
\end{document}
